{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "COS80024\n",
    "\n",
    "DATA SCIENCE PROJECT 1\n",
    "\n",
    "PROJECT 4: MOVIE RECOMMENDATION SYSTEM\n",
    "\n",
    "# S3.4.2: For memory-based technique based on users  (Executor: Promita)\n",
    "\n",
    "This task aims to select appropriate performance metrics and use them for model performance evaluation, comparison and analysis.\n",
    "\n",
    "Task Leader: Promita"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading relevant Python libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "import math\n",
    "import time\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model built without using surprise package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading ratings_small\n",
    "rating = pd.read_csv('ratings_small.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting list of users\n",
    "users_list = rating['userId'].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting list of movies\n",
    "movies_list = rating['movieId'].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculating mean rating\n",
    "mean_rate = rating['rating'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting userIds to index\n",
    "user2idx = {o:i for i, o in enumerate(users_list)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting movieIds to index\n",
    "movie2idx = {o:i for i, o in enumerate(movies_list)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Total number of users\n",
    "n_users = len(user2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Total number of movies\n",
    "n_movies = len(movie2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading files\n",
    "\n",
    "#File 1: uploading the training data\n",
    "train = pd.read_csv('train_df.csv')\n",
    "\n",
    "#File 2: uploading the testing data\n",
    "test = pd.read_csv('test_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove all information except User ID, Movie ID and Rating\n",
    "train = train[['userId','movieId','rating']]\n",
    "test = test[['userId','movieId','rating']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating matrix of the size of total number of users and total number of movies\n",
    "train_matrix = np.zeros((n_users, n_movies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(671, 9066)\n"
     ]
    }
   ],
   "source": [
    "#Filling up train_matrix with ratings for user,movie tuple\n",
    "for entry in train.itertuples():\n",
    "    user_idx = user2idx[entry[1]] #entry[1] refers to userId\n",
    "    movie_idx = movie2idx[entry[2]]  #entry[1] refers to movieId\n",
    "    train_matrix[user_idx, movie_idx] = entry[3]  #entry[1] refers to rating\n",
    "print(train_matrix.shape)\n",
    "mean_user_rating = train_matrix.mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Computing cosine silimarity between users\n",
    "user_similarity = np.cos(pairwise_distances(train_matrix, metric ='cosine'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to predict rating for user,movie tuple\n",
    "def cf_user(user, movie, k):\n",
    "    useridx = user2idx[user] \n",
    "    movieidx = movie2idx[movie] \n",
    "    mean = mean_user_rating[useridx]\n",
    "    movie_rateduser = train[train['movieId'] == movie]['userId'].tolist()\n",
    "    movie_rateduseridx = [user2idx[usr] for usr in movie_rateduser]\n",
    "    movie_rating = [train_matrix[usr][movieidx] for usr in movie_rateduseridx]\n",
    "    sim_idx = np.argsort(-user_similarity[useridx][movie_rateduseridx])[1:]\n",
    "    sim = user_similarity[useridx][movie_rateduseridx][sim_idx]\n",
    "    \n",
    "    if k>len(movie_rateduser):\n",
    "        k_sim = np.array(sim)\n",
    "        k_sim_rating = np.array([train_matrix[movie_rateduseridx[i]][movieidx] for i in sim_idx])\n",
    "    else:\n",
    "        k_sim = np.array(sim[:k])\n",
    "        k_sim_rating = np.array([train_matrix[movie_rateduseridx[i]][movieidx] for i in sim_idx[:k]])\n",
    "        \n",
    "    ratings_diff = k_sim_rating - mean\n",
    "    \n",
    "    pred = mean + k_sim.dot(ratings_diff)/(np.abs(k_sim).sum())\n",
    "    \n",
    "    if math.isnan(pred):\n",
    "        pred = mean\n",
    "    \n",
    "    return pred  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate performance of model\n",
    "def score(k):\n",
    "    test_ratings = np.array(test['rating'].tolist())\n",
    "    pred_test = []\n",
    "    for i, entry in enumerate(test.itertuples()):\n",
    "        user = entry[1]\n",
    "        movie = entry[2]\n",
    "        pred_test.append(cf_user(user,movie,k))\n",
    "    pred_test = np.array(pred_test)\n",
    "    print('RMSE: ',round(rmse(pred_test, test_ratings),4))\n",
    "    print('MAE: ',round(mae(pred_test, test_ratings),4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to calculate rmse\n",
    "def rmse(prediction, test_matrix):\n",
    "    return sqrt(mean_squared_error(prediction, test_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to calculate mae\n",
    "def mae(prediction, test_matrix):\n",
    "    return mean_absolute_error(prediction, test_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-15-bee601807ed2>:21: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  pred = mean + k_sim.dot(ratings_diff)/(np.abs(k_sim).sum())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE:  1.1564\n",
      "MAE:  0.8695\n"
     ]
    }
   ],
   "source": [
    "score(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model built using surprise package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Surprise libraries\n",
    "from surprise import KNNWithMeans\n",
    "from surprise import Reader\n",
    "from surprise import Dataset\n",
    "from surprise import accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data into a Surprise dataset\n",
    "reader = Reader(rating_scale = (1, 5))\n",
    "data_train = Dataset.load_from_df(train[['userId', 'movieId', 'rating']], reader)\n",
    "data_test = Dataset.load_from_df(test[['userId', 'movieId', 'rating']], reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build full trainset\n",
    "data_train = data_train.build_full_trainset()\n",
    "data_test = data_test.build_full_trainset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train rating 3.5399152683169963\n"
     ]
    }
   ],
   "source": [
    "mean = data_train.global_mean\n",
    "print('Train rating', mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test rating 3.645603576751118\n"
     ]
    }
   ],
   "source": [
    "mean = data_test.global_mean\n",
    "print('Test rating', mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the trainset and testset\n",
    "data_trainset = data_train.build_testset()\n",
    "data_testset = data_test.build_testset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create kNN algorithms with cosine similarity for user-based CF\n",
    "sim_options = {\n",
    "    \"name\": \"cosine\",\n",
    "    \"user_based\": True,  # Compute  similarities between users\n",
    "}\n",
    "algo = KNNWithMeans(sim_options=sim_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n"
     ]
    }
   ],
   "source": [
    "# Train the algorithm on the trainset\n",
    "user_based = algo.fit(data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing pickle file\n",
    "filename = 'user-based_CF.pickle'\n",
    "pickle.dump(user_based, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predicting for trainset\n",
    "train_pred = algo.test(data_trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.7842\n",
      "MAE:  0.5916\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5916013111205957"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate RMSE and MAE for trainset\n",
    "accuracy.rmse(train_pred)\n",
    "accuracy.mae(train_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting for testset\n",
    "test_pred = algo.test(data_testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.9336\n",
      "MAE:  0.7125\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7124530038365493"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate RMSE and MAE for testset\n",
    "accuracy.rmse(test_pred)\n",
    "accuracy.mae(test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions for trainset\n",
    "predict_train = []\n",
    "for i in range(len(train_pred)):\n",
    "    temp = train_pred[i].est\n",
    "    temp1 = round(temp,2)\n",
    "    predict_train.append(temp1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving predictions\n",
    "predict_train = pd.DataFrame(predict_train, columns=['predicted_rating'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting to dataframe\n",
    "predict_train_df = pd.DataFrame(predict_train, columns=['predicted_rating'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading onto train_df\n",
    "df1 = train.join(predict_train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Export the train dataframe with the predicted ratings \n",
    "df1.to_csv(\"train_df_cf_user.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting for testset\n",
    "predict_test = []\n",
    "for i in range(len(test_pred)):\n",
    "    temp = test_pred[i].est\n",
    "    temp1 = round(temp,2)\n",
    "    predict_test.append(temp1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving predictions\n",
    "predict_test_df = pd.DataFrame(predict_test, columns=['predicted_rating'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting to dataframe\n",
    "df2 = test.join(predict_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Export the test dataframe with the predicted ratings \n",
    "df2.to_csv(\"test_df_cf_user.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Selection\n",
    "\n",
    "Model without surprise package: RMSE score = 1.1864\n",
    "\n",
    "Model without surprise package: RMSE score = 0.9363\n",
    "\n",
    "Since, the model built with surprise package gives a better performance score, we will be using the surprise package to build the final model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
